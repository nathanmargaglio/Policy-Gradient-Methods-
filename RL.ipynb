{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "#%matplotlib widget\n",
    "mpl.style.use('dark_background')\n",
    "tf.enable_eager_execution()\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Methods for Reinforcement Learning with Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- For the decade preceding this paper (and still for some time after), the *value-based* approach dominated reinforcement learning.\n",
    "- This approach worked by estimating a value function and following a \"greedy\" policy, which is selecting an action leading to the highest possible reward\n",
    "- Value-based methods have limitations, though:\n",
    "    - It is oriented towards deterministic policies, whereas the optimal policy is often stochastic\n",
    "    - An arbitrarily small change in the estimated value of an action can cause it to be, or not be selected, causing issues with establishing convergence assurances  \n",
    "- Examples of such methods include Q-learning, Sarsa, and dynamic programming methods.\n",
    "\n",
    "In this paper, the authors explore an alternative approach to function approximation in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Approach\n",
    "\n",
    "- Rather than approximate a value function and using that to compute a deterministic policy, we approximate a stochastic policy directly\n",
    "- The policy can be represented as a neural network whose input is a representation of the state, whose output is action selection probabilities, and whose weights are policy parameters\n",
    "\n",
    "Let $\\theta$ denote the vector of policy parameters and $\\rho$ the performance of the corresponding policy (e.g., the average reward per step).  Then, in the *policy gradient* approach, the policy parameters are updated approximately proportional to the gradient:\n",
    "\n",
    "$$\\nabla\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}\\tag{1}$$\n",
    "\n",
    "where $\\alpha$ is a positive-definite step size.  If this can be achieved, then $\\theta$ can usually be assured to converge to a locally optimal policy in the performance measure $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "- state, action, and reward at ech time $t \\in \\{0, 1, 2, ...\\}$ are denoted $s_t \\in S$, $a_t \\in A$, and $r_t \\in \\mathbb{R}$\n",
    "- the environment's dynamics are characterized by state transition probabilities, $P^a_{ss'} = Pr\\{ s_{t+1} = s' \\mid s_t = s, a_t = a\\}$ and expected rewards $R^a_s = \\mathbb{E}\\{r_{t+1} \\mid s_t = s, a_t = a\\}, \\forall s, s' \\in S, a\\in A$\n",
    "- the agent's decision making procedure is characterized by a policy, $\\pi(s,a,\\theta) = Pr\\{a_t=a \\mid s_t=s,\\theta\\}, \\forall s\\in S, a\\in A$, where $\\theta \\in \\mathbb{R}^l$, for $l << \\left| S \\right|$, is a parameter vector.  We'll often just write $\\pi(s,a,\\theta)$ as $\\pi(s,a)$.\n",
    "- we assume $\\pi$ is differentiable with respect to its parameter, i.e., that $\\frac{\\partial\\pi(s,a)}{\\partial\\theta}$ exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def P(self, s, a):\n",
    "        pass\n",
    "    \n",
    "    def R(self, s, a):\n",
    "        pass\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pi(self, s, a):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start-State Formulation\n",
    "\n",
    "$$ \\rho(\\pi) = \\mathbb{E}\\{ \\sum_{t=1}^{\\infty} \\gamma^{t-1}r^t \\mid s_0,\\pi\\} $$\n",
    "\n",
    "$$ Q^\\pi(s,a) = \\mathbb{E}\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1}r_{t+k} \\mid s_t = s, a_t = a, \\pi \\} $$\n",
    "\n",
    "where $\\gamma \\in [0,1]$ is a discount rate.\n",
    "\n",
    "We define $d^\\pi (s)$ as a discounted weighting of states encountered starting at $s_0$ and the following $\\pi$:\n",
    "\n",
    "$$ d^\\pi(s) = \\sum_{t=0}^{\\infty} \\gamma^t Pr\\{s_t=s \\mid s_0,\\pi\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Theorem 1 (Policy Gradient)**\n",
    "\n",
    "For any MDP,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial \\theta}\n",
    "= \\sum_s d^\\pi (s)\n",
    "\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta} Q^\\pi(s,a)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "In words, this says the gradient of $\\rho$, the performance of our policy, with respect to $\\theta$, the parameters of our policy, is the sum of products of the gradient of our policy, $\\pi$, with respect to $\\theta$ and $Q^\\pi$, our value at the given state, for every possible action, multiplied by a weighing $d^\\pi$ summed over every possible state.\n",
    "\n",
    "Basically, if we can approximate $Q^\\pi$ given such a policy $\\pi$, then we can calculate the gradient of $\\rho$ to maximize the performance of our policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient with Appoximation\n",
    "\n",
    "If we can approximate $Q^\\pi$ by a learned function, we can use it in place of $Q^pi$ in the above theorem and still point roughly in the direction of the gradient.\n",
    "\n",
    "Let $f_w : S \\times A \\rightarrow \\mathbb{R}$ be our approximation to $Q^\\pi$ with parameter $w$.  It is natural to learn $f_w$ by following $\\pi$ and updating $w$ by a rule such as\n",
    "\n",
    "$$ \\Delta w_t \n",
    "\\; \\propto \\;\n",
    "\\frac{\\partial}{\\partial w}[\\hat{Q}^\\pi(s_t, a_t) - f_w (s_t, a_t)]^2 \n",
    "\\; \\propto \\;\n",
    "[ \\hat{Q}^\\pi (s_t, a_t) - f_w(s_t, a_t)] \\frac{\\partial f_w (s_t, a_t)}{\\partial w} $$\n",
    "\n",
    "where $\\hat{Q}^\\pi$ is some unbiased estimator of $Q^\\pi(s_t,a_t)$, perhaps $R_t$\n",
    "\n",
    "When such a process has converged to a local optimum, then\n",
    "\n",
    "$$\n",
    "\\sum_s d^\\pi(s) \\sum_a \\pi(s,a)[Q^\\pi (s,a) - f_w(s,a)] \\frac{\\partial f_w(s,a)}{\\partial w} = 0\n",
    "\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Theorem 2 (Policy Gradient with Function Approximation)**\n",
    "\n",
    "If $f_w$ satisfies (3) and is compatible with the policy parameterization in the sense that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_w (s,a)}{\\partial w} = \\frac{\\partial \\pi (s,a)}{\\partial \\theta} \\frac{1}{\\pi(s,a)}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial \\theta} = \\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} f_w(s,a)\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "This is equivalent to our first theorem, except in place of our state-value function $Q^\\pi$ we have our learned approximation $f_w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Deriving Algorithms\n",
    "\n",
    "Given a policy parameterization, Theorem 2 can be used to derive an appropriate form of the value-function parameterization.  Consider a policy that is a Gibbs distribution in a linear combination of features, i.e., softmax:\n",
    "\n",
    "$$\n",
    "\\pi(s,a) = \\frac{e^{\\theta^T\\phi_{sa}}}{\\sum_b e^{\\theta^T\\phi_{sb}}},\n",
    "\\forall s \\in S, a \\in A\n",
    "$$\n",
    "\n",
    "where each $\\phi_{sa}$ is an $l$-dimensional feature vector characterizing state-action pair $s,a$.  Meeting the compatibility condition (4) requires that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_w(s,a)}{\\partial w} = \\frac{\\partial \\pi (s,a)}{\\partial \\theta} \\frac{1}{\\pi(s,a)} = \\phi_{sa} - \\sum_b \\pi(s,b)\\phi_{sb}\n",
    "$$\n",
    "\n",
    "so that the natural parameterization of $f_w$ is\n",
    "\n",
    "$$\n",
    "f_w(s,a) = w^T[\\phi_{sa} - \\sum_b \\pi(s,b) \\phi_{sb} ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29719096]\n",
      "[0.34216155]\n",
      "[0.36064749]\n",
      "Sum: 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e556c1be99b44d1ba5693e4aaae2bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'bar',\n",
       "              'uid': '97c334e7-07c7-4aa4-9c5f-35ed07ca52a5',\n",
       "     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gibbs Distribution Policy\n",
    "\n",
    "theta = np.random.normal(0.5, 0.1, (6, 1))\n",
    "phi = lambda s, a: np.concatenate((s, a))\n",
    "\n",
    "states = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "\n",
    "actions = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "\n",
    "def pi(s, a):\n",
    "    num = np.exp(np.dot(theta.T, phi(s, a)))\n",
    "    den = np.sum([np.exp(np.dot(theta.T, phi(s, b))) for b in actions])\n",
    "    return num/den\n",
    "\n",
    "results = []\n",
    "for a in actions:\n",
    "    result = pi(states[0], a)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "print('Sum:', np.sum(results))\n",
    "\n",
    "data = go.Bar(\n",
    "    x=[f'$\\pi(s_0, a_{i})$' for i, a in enumerate(actions)],\n",
    "    y=np.array(results).squeeze()\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='$$\\pi(s_0, a)$$',\n",
    "    template='plotly_dark+presentation',\n",
    "    yaxis=dict(\n",
    "        range=[0,1]\n",
    "    )\n",
    ")\n",
    "\n",
    "go.FigureWidget(data=[data], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08016058]\n",
      "[-0.03533602]\n",
      "[-0.04010637]\n",
      "Sum: 0.004718187763855797\n",
      "Mean: 0.001572729254618599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6b6ea4b6ae46f394e46067b770dbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'bar',\n",
       "              'uid': 'b07bad6d-aea7-463d-b297-7c84cc98508f',\n",
       "     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.normal(0.5, 0.1, (6, 1))\n",
    "\n",
    "def f(s, a):\n",
    "    return np.dot(w.T, phi(s, a)) - np.dot(w.T, np.sum([ pi(s, b) * phi(s, b) for b in actions], axis=0))\n",
    "\n",
    "results = []\n",
    "for a in actions:\n",
    "    result = f(states[0], a)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "    \n",
    "print('Sum:', np.sum(results))\n",
    "print('Mean:', np.mean(results))\n",
    "\n",
    "data = go.Bar(\n",
    "    x=[f'$f_w(s_0, a_{i})$' for i, a in enumerate(actions)],\n",
    "    y=np.array(results).squeeze()\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='$$f_w(s_0, a)$$',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "go.FigureWidget(data=[data], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "\n",
    "The above form of $f_w$ requires that it have zero mean for each state: $\\sum_a \\pi(s,a) f_w(s,a) = 0, \\forall s\\in S$.  In this since, it's better to think of $f_w$ as an approximation of the *advantage* function, $A^\\pi (s,a) = Q^\\pi (s,a) - V^\\pi (s)$, rather than of $Q^\\pi$.\n",
    "\n",
    "We can generalize (5) to\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial \\theta} = \\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} [f_w(s,a) - v(s)]\n",
    "$$\n",
    "\n",
    "where $v: S \\rightarrow \\mathbb{R}$ is an arbitrary function.  The choice of $v$ doesn't affect any of these theorems, but can affect the variance of the gradient estimators.  In practice, $v$ should be set to the best available approximation of $V^\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Theorem 3 (Policy Iteration with Function Approximation)**\n",
    "\n",
    "Let $\\pi$ and $f_w$ be any differentiable function approximators for the policy and value function respectively that satisfy the compatibility condition (4) and for which\n",
    "\n",
    "$$\n",
    "\\max_{\\theta, s, a, i, j} \\left|\\frac{\\partial^2 \\pi(s,a)}{\\partial \\theta_i \\partial \\theta_j} \\right| < B < \\infty\n",
    "$$\n",
    "\n",
    "Let ${\\alpha_k}_{k=0}^{\\infty}$ be any step-size sequence such that $\\lim_{k\\rightarrow\\infty} \\alpha_k = 0$ and $\\sum_k \\alpha_k = \\infty$.  Then, for any MDP with bounded rewards, the sequence $\\{\\rho(\\pi_k)\\}_{k=0}^{\\infty}$, defined by any $\\theta_0, \\pi_k=\\pi(\\dot\\:,\\dot\\:,\\theta_k)$, and\n",
    "\n",
    "\\begin{align}\n",
    "& w_{k\\;} = w \\textrm{ such that } \\sum_s d^{\\pi k}(s) \\sum_a \\pi_k (s,a)[Q^{\\pi_k}(s,a) - f_w(s,a)]\\frac{\\partial f_w(s,a)}{\\partial w} = 0\n",
    "\\\\\n",
    "& \\theta_{k+1} = \\theta_k + \\alpha_k \\sum_s d^{\\pi_k}(s) \\sum_a \\frac{\\partial \\pi_k (s,a)}{\\partial \\theta} f_{w_k} (s,a)\n",
    "\\end{align}\n",
    "\n",
    "converges such that $\\lim_{k\\rightarrow \\infty} \\frac{\\partial \\rho(\\pi_k)}{\\partial\\theta} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating $\\theta$ through $\\pi$ and $f_w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test these theorems using code.  At a high level, what we want to accomplish is:\n",
    "- Defining our action space $A$ and state space $S$\n",
    "- Initializing $\\theta$ and $w$\n",
    "- Defining our $\\pi$ and $f_w$ functions\n",
    "- Defining our $\\nabla\\pi$ and $\\nabla f_w$ functions\n",
    "- Defining our training function, which incorporates the update rule for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $A$ and $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define our state and action spaces.  In order to easily visualize this procedure, we'll need to keep these spaces simple.  Here, the action space $A$ is discrete and of cardinality 3.  We represent each state as a one-hot encoding.  Similarly, the state space is $S$ is discrete and of cardinality.  We give it a similar treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "states = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\theta$, $w$, and $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize $\\theta$ and $\\w$ as arrays of length 6 where each entry is a random value between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b042c06d3d641f0b2eac17463412dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'type': 'bar',\n",
       "              'uid': '4feb1921-7d3e-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta = np.random.normal(0.5, 0.1, (len(states) + len(actions), 1))\n",
    "w = np.random.normal(0.5, 0.1, (6, 1))\n",
    "phi = lambda s, a: np.concatenate((s, a))\n",
    "\n",
    "def f(s, a):\n",
    "    result = np.dot(w.T, phi(s, a)) - np.dot(w.T, np.sum([ pi(s, b) * phi(s, b) for b in actions], axis=0))\n",
    "    return result\n",
    "\n",
    "def f_grad(s, a, Q):\n",
    "    df = phi(s, a) - np.sum([ pi(s, b) * phi(s, b) for b in actions], axis=0)\n",
    "    result = (Q(s,a) - f(s,a))*df\n",
    "    return result\n",
    "\n",
    "def pi(s, a):\n",
    "    num = np.exp(np.dot(theta.T, phi(s, a)))\n",
    "    den = np.sum([np.exp(np.dot(theta.T, phi(s, b))) for b in actions])\n",
    "    return num/den\n",
    "\n",
    "def prob(s, a):\n",
    "    sm = np.zeros((len(a),1))\n",
    "    for i, a in enumerate(actions):\n",
    "        sm[i] = pi(s, a)\n",
    "    return sm\n",
    "\n",
    "def pi_grad(s, a):\n",
    "    left = phi(s, a)*pi(s, a)\n",
    "    right = np.exp(np.dot(theta.T, phi(s, a))) * np.sum([ phi(s,b)*np.exp(np.dot(theta.T, phi(s, b))) for b in actions])\n",
    "    diff = left - right\n",
    "    den = (np.sum([ np.exp(np.dot(theta.T, phi(s, b))) for b in actions ]))**2\n",
    "    return diff/den\n",
    "\n",
    "verbose = False\n",
    "def train_value(action):\n",
    "    global fig\n",
    "    global theta\n",
    "    alpha = 1.\n",
    "    iters = 10\n",
    "\n",
    "    for i in range(iters):\n",
    "        grad = pi_grad(states[state], actions[action]).reshape(theta.shape)\n",
    "        theta += alpha*grad*f(states[state], actions[action])\n",
    "\n",
    "    fig.data[0]['y'] = np.array([pi(states[state], a) for a in actions]).flatten()\n",
    "    \n",
    "    with out:\n",
    "        if verbose:\n",
    "            print(action)\n",
    "\n",
    "# setup plotting\n",
    "\n",
    "layout_params = dict(\n",
    "    title=\"$\\pi(s_0,a)$\",\n",
    "    yaxis=dict(\n",
    "        range=[0,1]\n",
    "    ),\n",
    "    template=\"plotly_dark+presentation\"\n",
    ")\n",
    "layout = go.Layout(**layout_params)\n",
    "\n",
    "data = []\n",
    "data.append(\n",
    "    go.Bar(\n",
    "        x=[f'$\\pi(s_0, a_{i})$' for i in range(len(actions))],\n",
    "        y=np.array([pi(states[state],a) for a in actions]).flatten()\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.FigureWidget(data=data, layout=layout)\n",
    "\n",
    "# value function\n",
    "results = []\n",
    "for a in actions:\n",
    "    result = f(states[0], a)\n",
    "    results.append(result)\n",
    "\n",
    "value_data = go.Bar(\n",
    "    x=[f'$f_w(s_0, a_{i})$' for i, a in enumerate(actions)],\n",
    "    y=np.array(results).squeeze()\n",
    ")\n",
    "\n",
    "value_layout = go.Layout(\n",
    "    title='$$f_w(s_0, a)$$',\n",
    "    template='plotly_dark+presentation'\n",
    ")\n",
    "\n",
    "value_widget = go.FigureWidget(data=[value_data], layout=value_layout)\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "# interaction\n",
    "\n",
    "state_dropdown = widgets.Dropdown(\n",
    "    options=[f'State {s}' for s in range(3)],\n",
    "    description='State:',\n",
    "    layout=widgets.Layout(\n",
    "        width='25%'\n",
    "    )\n",
    ")\n",
    "\n",
    "def dropdown_change(change):\n",
    "    global state\n",
    "    if change['name'] == 'index':\n",
    "        with out:\n",
    "            state = change['new']\n",
    "\n",
    "state_dropdown.observe(dropdown_change)\n",
    "\n",
    "options_box = widgets.HBox(\n",
    "    [\n",
    "        state_dropdown\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        justify_content='space-around',\n",
    "        margin='10px 0'\n",
    "    )\n",
    ")\n",
    "\n",
    "buttons = []\n",
    "for i, a in enumerate(actions):\n",
    "    button = widgets.Button(\n",
    "        description=f'a_{i}',\n",
    "        state=i\n",
    "    )\n",
    "    button_function = lambda state: lambda b: train_value(state)\n",
    "    button.on_click(button_function(i))\n",
    "    buttons.append(button)\n",
    "\n",
    "button_box = widgets.HBox(\n",
    "    buttons,\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        justify_content='space-around',\n",
    "        margin='10px 0'\n",
    "    )\n",
    ")\n",
    "\n",
    "verbose = False\n",
    "widgets.VBox([\n",
    "    widgets.HBox(\n",
    "        [fig, value_widget],\n",
    "        layout=widgets.Layout(\n",
    "            width='100%',\n",
    "            justify_content='space-around',\n",
    "            margin='10px 0'\n",
    "        )\n",
    "    ),\n",
    "    button_box,\n",
    "    out\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.t_limit = 5\n",
    "        self.states = [\n",
    "            [1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ]\n",
    "        self.transition_probs = [\n",
    "            [\n",
    "                [ 0.1, 0.7, 0.2 ],\n",
    "                [ 0.1, 0.7, 0.2 ],\n",
    "                [ 0.1, 0.7, 0.2 ]\n",
    "            ],\n",
    "            [\n",
    "                [ 0.3, 0.4, 0.3 ],\n",
    "                [ 0.3, 0.4, 0.3 ],\n",
    "                [ 0.3, 0.4, 0.3 ]\n",
    "            ],\n",
    "            [\n",
    "                [ 0.5, 0.2, 0.3 ],\n",
    "                [ 0.5, 0.2, 0.3 ],\n",
    "                [ 0.5, 0.2, 0.3 ]\n",
    "            ]\n",
    "        ]\n",
    "        self.reward_probs = [\n",
    "            [ -1.0, 1.0, 0.0 ],\n",
    "            [ 1.0, 0.0, -1.0 ],\n",
    "            [ 0.0, -1.0, 1.0 ]\n",
    "        ]\n",
    "    \n",
    "    def P(self, s, a):\n",
    "        return self.transition_probs[s][a]\n",
    "    \n",
    "    def R(self, s, a):\n",
    "        return self.reward_probs[s][a]\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(3, p=self.P(self.current_state, action))\n",
    "        reward = self.R(self.current_state, action)\n",
    "        \n",
    "        self.t += 1\n",
    "        done = False\n",
    "        if self.t >= self.t_limit:\n",
    "            done = True\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return obs, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.current_state = 0\n",
    "        return self.states[self.current_state]\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, obs_size=3):\n",
    "        self.actions = [\n",
    "            [1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ]\n",
    "        self.theta = np.random.normal(0.5, 0.1, (obs_size + len(actions), 1))\n",
    "        \n",
    "    def pi(self, s, a):\n",
    "        num = np.exp(np.dot(self.theta.T, phi(s, a)))\n",
    "        den = np.sum([np.exp(np.dot(self.theta.T, phi(s, b))) for b in actions])\n",
    "        return num/den\n",
    "    \n",
    "    def step(self, observation):\n",
    "        probs = np.array([self.pi(observation, action) for action in self.actions]).flatten()\n",
    "        return np.random.choice(len(probs), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 0, a: 0\n",
      "r: -1.0\n",
      "\n",
      "s: 2, a: 2\n",
      "r: 1.0\n",
      "\n",
      "s: 2, a: 2\n",
      "r: 1.0\n",
      "\n",
      "s: 1, a: 2\n",
      "r: -1.0\n",
      "\n",
      "s: 1, a: 2\n",
      "r: -1.0\n",
      "\n",
      "Total Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = agent.step(obs)\n",
    "    print(f\"s: {env.current_state}, a: {action}\")\n",
    "    obs, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"r: {reward}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
